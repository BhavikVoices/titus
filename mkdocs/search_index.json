{
    "docs": [
        {
            "location": "/", 
            "text": "Titus\n\n\nTitus is the Netflix Cloud Container Runtime that manages containers and provides integrations to the infrastructure ecosystem.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#titus", 
            "text": "Titus is the Netflix Cloud Container Runtime that manages containers and provides integrations to the infrastructure ecosystem.", 
            "title": "Titus"
        }, 
        {
            "location": "/build/", 
            "text": "Agent\n\n\nTitus-vpc-driver\n\n\nSee the build instructions in the \nrepo\n.\n\n\nTitus-executor\n\n\nSee the build instructions in the \nrepo\n.\n\n\nTitus Api Definitions\n\n\ngit clone https://github.com/Netflix/titus-api-definitions.git\ncd titus-api-definitions\n./gradlew clean build\n\n\n\n\nTitus Master\n\n\ngit clone https://github.com/Netflix/titus-control-plane.git\ncd titus-control-plane\n./gradlew clean buildDeb -PidlLocal\n\n\n\n\nTitus Gateway\n\n\ngit clone https://github.com/Netflix/titus-control-plane.git\ncd titus-control-plane\n./gradlew clean buildDeb -PidlLocal\n\n\n\n\nNote:\n the \ntitus-api-definitions\n git repo must be in the same root folder as \ntitus-control-plane\n git repo in order to build the master and gateway components with \n-PidlLocal\n.\nRunning all builds will produce 5 debs under the build/distributions folder in each component.", 
            "title": "Building the Code"
        }, 
        {
            "location": "/build/#agent", 
            "text": "", 
            "title": "Agent"
        }, 
        {
            "location": "/build/#titus-vpc-driver", 
            "text": "See the build instructions in the  repo .", 
            "title": "Titus-vpc-driver"
        }, 
        {
            "location": "/build/#titus-executor", 
            "text": "See the build instructions in the  repo .", 
            "title": "Titus-executor"
        }, 
        {
            "location": "/build/#titus-api-definitions", 
            "text": "git clone https://github.com/Netflix/titus-api-definitions.git\ncd titus-api-definitions\n./gradlew clean build", 
            "title": "Titus Api Definitions"
        }, 
        {
            "location": "/build/#titus-master", 
            "text": "git clone https://github.com/Netflix/titus-control-plane.git\ncd titus-control-plane\n./gradlew clean buildDeb -PidlLocal", 
            "title": "Titus Master"
        }, 
        {
            "location": "/build/#titus-gateway", 
            "text": "git clone https://github.com/Netflix/titus-control-plane.git\ncd titus-control-plane\n./gradlew clean buildDeb -PidlLocal  Note:  the  titus-api-definitions  git repo must be in the same root folder as  titus-control-plane  git repo in order to build the master and gateway components with  -PidlLocal .\nRunning all builds will produce 5 debs under the build/distributions folder in each component.", 
            "title": "Titus Gateway"
        }, 
        {
            "location": "/install/prereqs/", 
            "text": "Required\n\n\nLinux\n\n\nWe currently run our agents on Ubuntu Xenial (16.04.2) with kernel 4.9.34 with\npatches for EFS. Until Titus open source supports EFS, it is likely that Xenial\nand a 4.9 kernel is sufficient.\n\n\nWe run all other tiers of Titus on Trusty.\n\n\nMesos and Zookeeper\n\n\n\n\nZookeeper 3.4.8 (with Exhibitor 1.5.5)\n\n\nMesos 1.0.1\n\n\n\n\nWe run a high availability configuration of Mesos and Zookeeper. You can run this\non a node using Docker in a non-production mode by using:\n\n\nsudo docker run -d -p 2181:2181 --name zookeeper jplock/zookeeper:3.4.10\nsudo docker run -d -p 5050:5050 --net=host --name mesomaster mesosphere/mesos-master:1.0.1-2.0.93.ubuntu1404 mesos-master --zk=zk://localhost:2181/titus/mainvpc/mesos --work_dir=/tmp/master --log_dir=/var/log/mesos --logging_level=INFO --quorum=1\n\n\n\n\nYou can see the key ports for these are 2181 (Zookeeper) and 5050 (Mesos).\n\n\nSpinnaker\n\n\nWe deploy Titus via Spinnaker to an EC2 cloud provider. Eventually we may\nrelease these pipelines, but for now we suggest direct deployments using\nthe AWS EC2 console.\n\n\nDocker Registry\n\n\nWe operate a docker registry based on Docker Distribution. We have tested with\nDockerhub and suggest this or the EC2 Container Registry Service (ECR).\n\n\nAWS Configuration\n\n\nIAM Role and security group for Titus are \ndocumented\n.\n\n\nOptional\n\n\nWe use Cassandra as our persistence store for Titus master. For OSS, we\ncurrent suggest the in-memory store.\n\n\nWe have integration with Eureka to disable agents and monitor health. For now,\nthis is not part of the OSS implementation.\n\n\nWe write all task state updates to Elastic Search for operational insight. For\nnow, this is not part of the OSS implementation.\n\n\nWe use Edda to understand which ASG's are available in Titus. For now, this is\nnot part of the OSS implementation.", 
            "title": "Prereqs"
        }, 
        {
            "location": "/install/prereqs/#required", 
            "text": "", 
            "title": "Required"
        }, 
        {
            "location": "/install/prereqs/#linux", 
            "text": "We currently run our agents on Ubuntu Xenial (16.04.2) with kernel 4.9.34 with\npatches for EFS. Until Titus open source supports EFS, it is likely that Xenial\nand a 4.9 kernel is sufficient.  We run all other tiers of Titus on Trusty.", 
            "title": "Linux"
        }, 
        {
            "location": "/install/prereqs/#mesos-and-zookeeper", 
            "text": "Zookeeper 3.4.8 (with Exhibitor 1.5.5)  Mesos 1.0.1   We run a high availability configuration of Mesos and Zookeeper. You can run this\non a node using Docker in a non-production mode by using:  sudo docker run -d -p 2181:2181 --name zookeeper jplock/zookeeper:3.4.10\nsudo docker run -d -p 5050:5050 --net=host --name mesomaster mesosphere/mesos-master:1.0.1-2.0.93.ubuntu1404 mesos-master --zk=zk://localhost:2181/titus/mainvpc/mesos --work_dir=/tmp/master --log_dir=/var/log/mesos --logging_level=INFO --quorum=1  You can see the key ports for these are 2181 (Zookeeper) and 5050 (Mesos).", 
            "title": "Mesos and Zookeeper"
        }, 
        {
            "location": "/install/prereqs/#spinnaker", 
            "text": "We deploy Titus via Spinnaker to an EC2 cloud provider. Eventually we may\nrelease these pipelines, but for now we suggest direct deployments using\nthe AWS EC2 console.", 
            "title": "Spinnaker"
        }, 
        {
            "location": "/install/prereqs/#docker-registry", 
            "text": "We operate a docker registry based on Docker Distribution. We have tested with\nDockerhub and suggest this or the EC2 Container Registry Service (ECR).", 
            "title": "Docker Registry"
        }, 
        {
            "location": "/install/prereqs/#aws-configuration", 
            "text": "IAM Role and security group for Titus are  documented .", 
            "title": "AWS Configuration"
        }, 
        {
            "location": "/install/prereqs/#optional", 
            "text": "We use Cassandra as our persistence store for Titus master. For OSS, we\ncurrent suggest the in-memory store.  We have integration with Eureka to disable agents and monitor health. For now,\nthis is not part of the OSS implementation.  We write all task state updates to Elastic Search for operational insight. For\nnow, this is not part of the OSS implementation.  We use Edda to understand which ASG's are available in Titus. For now, this is\nnot part of the OSS implementation.", 
            "title": "Optional"
        }, 
        {
            "location": "/install/prereqs-amazon/", 
            "text": "Subnets\n\n\nThe Titus agent must run in a \"private subnet\" (Amazon terminology), or a Subnet which does not rely on 1:1 NAT for communication across the VPC boundary. This is because the Titus VPC integration code sets up agent IPs in the same subnet that the agent is spawned in, and the containers use secondary Private IPs. The driver does not set up any kind of NAT on these secondary IPs, so they are without ingress connectivity.\n\n\nVPC Setup\n\n\nIn the most minimal Titus install, you need to create a VPC with a \"public\" and \"private\" subnet. All of your Titus infrastructure will live in the private subnet.\n\n\nYou will not be able to SSH directly into your Titus instances. You need to create a instance with the \ntitusmaster-mainvpc\n and \ntitusbastions\n security groups. This instance should be provisioned in the \nPublic\n subnet(s). You can then go ahead and SSH to this instance with the \n-A\n option. \n\n\nThe default Amazon VPC wizard will create this for you. If you need to scale your Titus cluster, you need to copy the configuration of the \nprivate\n subnets, and create one per availability zone.\n\n\nA slightly more advanced version of this configuration is to configure IPv6 on the private subnet, and use an igw for the IPv6 routing for that subnet, so you can SSH directly to the Titus infrastructure on their IPv6 addresses.\n\n\nCreating security groups\n\n\nThree security groups are needed. We are naming them \ntitusbastion\n, \ntitusapp\n, \ntitusmaster-mainvpc\n:\n\n\n\n\nBastion Security Group\n\n\nFor inbound:\n\n\n\n\nFrom anywhere, TCP 22\n\n\nFrom anywhere, All ICMP\n\n\n\n\nInfrastructure security group\n\n\nThis is for the \ntitusmaster-mainvpc\n security group\n\n\nFor inbound\n\n\n\n\nFrom titusmaster-mainvpc security group, ALL TCP, All ICMP\n\n\nFrom anywhere (including Internet), SSH\n\n\n\n\n\n\nFor outbound\n\n\n\n\nAll traffic\n\n\n\n\n\n\nApp security group\n\n\nThis is for the \ntitusapp\n \n\n\nFor inbound and outbound\n\n\n\n\nUp to your application needs\n\n\n\n\nCreating IAM Roles\n\n\nThree IAM roles are needed. We are naming them `titusmasterInstanceProfile' and 'titusappwiths3InstanceProfile'\nand 'titusappnos3InstanceProfile'.\n\n\n\n\nInfrastructure IAM Role\n\n\nFor now, we are using a wide open IAM role. We can refine this later.\n\n\n\n\nApp IAM Roles\n\n\nYou need to allow this IAM Role to be assumed into via the Infrastructure Role. You can do this by setting\nup trusted relationships. It should look like this:\n\n\n\n\nThe Trust relationship should look like:\n\n\n{\n  \nVersion\n: \n2012-10-17\n,\n  \nStatement\n: [\n    {\n      \nEffect\n: \nAllow\n,\n      \nPrincipal\n: {\n        \nService\n: \nec2.amazonaws.com\n\n      },\n      \nAction\n: \nsts:AssumeRole\n\n    },\n    {\n      \nEffect\n: \nAllow\n,\n      \nPrincipal\n: {\n        \nAWS\n: \narn:aws:iam::ACCOUNTID:role/titusmasterInstanceProfile\n\n      },\n      \nAction\n: \nsts:AssumeRole\n\n    }\n  ]\n}\n\n\n\n\nFor permissions, pick two sets of permissions that matter to your apps. We created one with S3 read access\nand one without to be able to test the IAM role support feature.\n\n\nOverview of the applied IAM Roles\n\n\nEventually this is how the IAM roles your created will be used", 
            "title": "Amazon Prereqs"
        }, 
        {
            "location": "/install/prereqs-amazon/#subnets", 
            "text": "The Titus agent must run in a \"private subnet\" (Amazon terminology), or a Subnet which does not rely on 1:1 NAT for communication across the VPC boundary. This is because the Titus VPC integration code sets up agent IPs in the same subnet that the agent is spawned in, and the containers use secondary Private IPs. The driver does not set up any kind of NAT on these secondary IPs, so they are without ingress connectivity.", 
            "title": "Subnets"
        }, 
        {
            "location": "/install/prereqs-amazon/#vpc-setup", 
            "text": "In the most minimal Titus install, you need to create a VPC with a \"public\" and \"private\" subnet. All of your Titus infrastructure will live in the private subnet.  You will not be able to SSH directly into your Titus instances. You need to create a instance with the  titusmaster-mainvpc  and  titusbastions  security groups. This instance should be provisioned in the  Public  subnet(s). You can then go ahead and SSH to this instance with the  -A  option.   The default Amazon VPC wizard will create this for you. If you need to scale your Titus cluster, you need to copy the configuration of the  private  subnets, and create one per availability zone.  A slightly more advanced version of this configuration is to configure IPv6 on the private subnet, and use an igw for the IPv6 routing for that subnet, so you can SSH directly to the Titus infrastructure on their IPv6 addresses.", 
            "title": "VPC Setup"
        }, 
        {
            "location": "/install/prereqs-amazon/#creating-security-groups", 
            "text": "Three security groups are needed. We are naming them  titusbastion ,  titusapp ,  titusmaster-mainvpc :", 
            "title": "Creating security groups"
        }, 
        {
            "location": "/install/prereqs-amazon/#bastion-security-group", 
            "text": "For inbound:   From anywhere, TCP 22  From anywhere, All ICMP", 
            "title": "Bastion Security Group"
        }, 
        {
            "location": "/install/prereqs-amazon/#infrastructure-security-group", 
            "text": "This is for the  titusmaster-mainvpc  security group  For inbound   From titusmaster-mainvpc security group, ALL TCP, All ICMP  From anywhere (including Internet), SSH    For outbound   All traffic", 
            "title": "Infrastructure security group"
        }, 
        {
            "location": "/install/prereqs-amazon/#app-security-group", 
            "text": "This is for the  titusapp    For inbound and outbound   Up to your application needs", 
            "title": "App security group"
        }, 
        {
            "location": "/install/prereqs-amazon/#creating-iam-roles", 
            "text": "Three IAM roles are needed. We are naming them `titusmasterInstanceProfile' and 'titusappwiths3InstanceProfile'\nand 'titusappnos3InstanceProfile'.", 
            "title": "Creating IAM Roles"
        }, 
        {
            "location": "/install/prereqs-amazon/#infrastructure-iam-role", 
            "text": "For now, we are using a wide open IAM role. We can refine this later.", 
            "title": "Infrastructure IAM Role"
        }, 
        {
            "location": "/install/prereqs-amazon/#app-iam-roles", 
            "text": "You need to allow this IAM Role to be assumed into via the Infrastructure Role. You can do this by setting\nup trusted relationships. It should look like this:   The Trust relationship should look like:  {\n   Version :  2012-10-17 ,\n   Statement : [\n    {\n       Effect :  Allow ,\n       Principal : {\n         Service :  ec2.amazonaws.com \n      },\n       Action :  sts:AssumeRole \n    },\n    {\n       Effect :  Allow ,\n       Principal : {\n         AWS :  arn:aws:iam::ACCOUNTID:role/titusmasterInstanceProfile \n      },\n       Action :  sts:AssumeRole \n    }\n  ]\n}  For permissions, pick two sets of permissions that matter to your apps. We created one with S3 read access\nand one without to be able to test the IAM role support feature.", 
            "title": "App IAM Roles"
        }, 
        {
            "location": "/install/prereqs-amazon/#overview-of-the-applied-iam-roles", 
            "text": "Eventually this is how the IAM roles your created will be used", 
            "title": "Overview of the applied IAM Roles"
        }, 
        {
            "location": "/install/master/", 
            "text": "Master\n\n\nRun as ubuntu user\n\n\n\n\nadd titus apt repo with \ncurl -s https://8095c452e9473a3fae3ea86a6f2572c2cde0d7b5ec63e84f:@packagecloud.io/install/repositories/netflix/titus/script.deb.sh | sudo bash\n\n\nupdate apt repos with \nsudo apt-get update\n\n\ninstall java8 with \nsudo apt-get install openjdk-8-jdk\n\n\ninstall mesos with \nsudo apt-get install mesos\n\n\nCopy \ntitus-server-master/build/distributions/titus-server-master\nversion\n.deb\n to server\n\n\n\n\nRun \nsudo dpkg -i titus-server-master\nversion\n.deb\n to install the debian\n\n\n\n\n\n\nCreate \n~/titusmaster.properties\n with the properties:\n\n\n\n\n\n\ntitus.master.apiport=7001\ntitus.master.apiProxyPort=7001\ntitus.master.grpcServer.port=7104\n\ntitus.zookeeper.connectString=\nZK_IP\n:\nZK_PORT\n\ntitus.zookeeper.root=/titus/main\n\nmesos.master.location=\nMESOS_MASTER_IP\n:\nMESOS_MASTER_PORT\n\n\ntitus.agent.fullCacheRefreshIntervalMs=10000\ntitus.agent.agentServerGroupPattern=.*\n\ntitusMaster.job.configuration.defaultIamRole=\nDEFAULT_IAM_ROLE_ARN\n\ntitusMaster.job.configuration.defaultSecurityGroups=\nSECURITY_GROUP_ID\n\n\nmesos.titus.executor=/apps/titus-executor/bin/titus-executor\n\n\n\n\n\n\nStart server with \nsudo /opt/titus-server-master/bin/titus-server-master -p ~/titusmaster.properties | tee ~/titusmaster.log\n\n\n\n\nGateway\n\n\nRun as ubuntu user\n\n\n\n\nupdate apt repos with \nsudo apt-get update\n\n\n\n\ninstall java8 with \nsudo apt-get install openjdk-8-jdk\n\n\n\n\n\n\nCopy \ntitus-server-gateway/build/distributions/titus-server-gateway\nversion\n.deb\n to server\n\n\n\n\n\n\nRun \nsudo dpkg -i titus-server-gateway\nversion\n.deb\n to install the debian\n\n\n\n\n\n\nCreate \n~/titusgateway.properties\n with the properties:\n\n\n\n\n\n\ntitus.gateway.masterIp=\nMASTER_IP\n\ntitus.gateway.masterHttpPort=\nMASTER_PORT\n\n\n\n\n\n\n\nStart server with \nsudo /opt/titus-server-gateway/bin/titus-server-gateway -p ~/titusgateway.properties | tee ~/titusgateway.log", 
            "title": "Master and Gateway"
        }, 
        {
            "location": "/install/master/#master", 
            "text": "Run as ubuntu user   add titus apt repo with  curl -s https://8095c452e9473a3fae3ea86a6f2572c2cde0d7b5ec63e84f:@packagecloud.io/install/repositories/netflix/titus/script.deb.sh | sudo bash  update apt repos with  sudo apt-get update  install java8 with  sudo apt-get install openjdk-8-jdk  install mesos with  sudo apt-get install mesos  Copy  titus-server-master/build/distributions/titus-server-master version .deb  to server   Run  sudo dpkg -i titus-server-master version .deb  to install the debian    Create  ~/titusmaster.properties  with the properties:    titus.master.apiport=7001\ntitus.master.apiProxyPort=7001\ntitus.master.grpcServer.port=7104\n\ntitus.zookeeper.connectString= ZK_IP : ZK_PORT \ntitus.zookeeper.root=/titus/main\n\nmesos.master.location= MESOS_MASTER_IP : MESOS_MASTER_PORT \n\ntitus.agent.fullCacheRefreshIntervalMs=10000\ntitus.agent.agentServerGroupPattern=.*\n\ntitusMaster.job.configuration.defaultIamRole= DEFAULT_IAM_ROLE_ARN \ntitusMaster.job.configuration.defaultSecurityGroups= SECURITY_GROUP_ID \n\nmesos.titus.executor=/apps/titus-executor/bin/titus-executor   Start server with  sudo /opt/titus-server-master/bin/titus-server-master -p ~/titusmaster.properties | tee ~/titusmaster.log", 
            "title": "Master"
        }, 
        {
            "location": "/install/master/#gateway", 
            "text": "Run as ubuntu user   update apt repos with  sudo apt-get update   install java8 with  sudo apt-get install openjdk-8-jdk    Copy  titus-server-gateway/build/distributions/titus-server-gateway version .deb  to server    Run  sudo dpkg -i titus-server-gateway version .deb  to install the debian    Create  ~/titusgateway.properties  with the properties:    titus.gateway.masterIp= MASTER_IP \ntitus.gateway.masterHttpPort= MASTER_PORT    Start server with  sudo /opt/titus-server-gateway/bin/titus-server-gateway -p ~/titusgateway.properties | tee ~/titusgateway.log", 
            "title": "Gateway"
        }, 
        {
            "location": "/install/agent/", 
            "text": "Using Cloud-init\n\n\nWe recommend that you use the following cloud-init config with Ubuntu 16.04. Just make sure you've added an IAM role to your profile: \ncloud-init\n. There are a few fields we recommend filling in for yourself here. For example, you should fill in your SSH keys, and not ours. Also, you should configure the \n/etc/mesos-attributes.sh\n config file that's included in the cloud-init to be suited for your instance. The mesos-specific configuration that needs to be adjusted is the \nstack\n, \nasg\n, and \nRes\n attributes. You will also need to edit the run_cmd section, and change \nzk://172.31.0.136:2181/titus/mainvpc/mesos\n, based on your Zookeeper configuration. The specifics of the \nRes\n attribute, and how it refers to how your ENIs work is explained below.\n\n\nIn order to use cloud-init, you need to go ahead and paste the linked yaml file into your Ubuntu 16.04 VM's user data.\n\n\nManual Install\n\n\nThis is for advanced users only. We do not recommend using these instructions unless you're running a customized system. We also recommend looking at the cloud-init as the reference for canonical configuration files.\n\n\nLinux\n\n\nWe recommend Ubuntu Linux, Xenial, with a 4.4 or newer version for running Titus. We also recommend this for developing Titus agent itself. We recommend using XFS or Btrfs for the storage of the Docker daemon itself, and separating out the storage of the agent \"Control Plane\" components versus the non-control plane components. \n\n\nInstall Packagecloud Repo\n\n\nRun the following commands:\n\n\ncurl -s https://8095c452e9473a3fae3ea86a6f2572c2cde0d7b5ec63e84f:@packagecloud.io/install/repositories/netflix/titus/script.deb.sh | sudo bash\napt-get update\n\n\n\n\nInstall Docker\n\n\nInstall docker as instructed \nhere\n\n\n\n\nAdd the current user to docker group \nsudo gpasswd -a $USER docker\n, log out and then log back in.\n\n\n\n\nYou can add the following drop-in systemd unit to configure Docker for your needs:\n\n\n# /etc/systemd/system/docker.service.d/10-docker-config.conf\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd --log-level=debug -H fd:// --init-path=/apps/titus-executor/bin/tini-static --iptables=false --storage-driver=overlay2\nRestart=always\nStartLimitInterval=0\nRestartSec=5\n\n\n\n\nYou should also configure the Docker TCP Proxy, with the following systemd units:\n\n\ndocker-tcp-proxy.service\n\n\n# /lib/systemd/system/docker-tcp-proxy.service\n[Unit]\nDescription=Docker Unix Socket TCP Proxy\nRequires=docker.socket\nRequires=docker-tcp-proxy.socket\nAfter=docker.socket\nAfter=docker-tcp-proxy.socket\n\n[Service]\nExecStart=/lib/systemd/systemd-socket-proxyd /var/run/docker.sock\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n\ndocker-tcp-proxy.socket\n\n\n# /lib/systemd/system/docker-tcp-proxy.socket\n[Unit]\nRequires=docker.socket\nAfter=docker.socket\n\n[Socket]\nListenStream=0.0.0.0:4243\n\n[Install]\nWantedBy=sockets.target\n\n\n\n\nOnce you've done this, we recommend restarting docker, enabling these systemd units to restart on every boot.\n\n\nMesos-agent\n\n\nYou can download Mesos from Mesosphere's OSS site here: https://open.mesosphere.com/downloads/mesos/#apache-mesos-1.1.3, we recommend installing 1.1.3. You can download the deb directly \nhttp://repos.mesosphere.com/ubuntu/pool/main/m/mesos/mesos_1.1.3-2.0.1.ubuntu1604_amd64.deb\n.\n\n\nUnfortunately, Mesos does not come with a systemd unit file out of the box, so we have one that you can adapt to your own needs:\n\n\n# /lib/systemd/system/mesos-agent.service\n[Unit]\nDescription=Mesos\nWants=docker.service\nAfter=docker.service\nConflicts=halt.target shutdown.target sigpwr.target\n\n[Service]\nExecStartPre=/bin/mkdir -p /var/lib/mesos\nEnvironmentFile=/etc/mesos-agent.config\nExecStart=/usr/sbin/mesos-agent\nExecStopPost=/bin/rm -rf /var/lib/mesos/meta/slaves/latest\nRestart=always\nStartLimitInterval=0\nRestartSec=5\nLimitNOFILE=65535\nKillMode=mixed\nKillSignal=SIGUSR1\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n\nOur \n/etc/mesos-agent.config\n includes the following:\n\n\nMESOS_MASTER=zk://1.1.1.1:2181,2.2.2.2:2181/titus/devvpc7/mesos\nMESOS_PORT=7101\nMESOS_RECOVER=reconnect\nMESOS_WORK_DIR=/var/lib/mesos\nMESOS_STRICT=false\nMESOS_RESOURCES=ports:[7150-7200];mem:230400;disk:523264;network:9000\nMESOS_ATTRIBUTES=region:us-east-1;asg:titusagent-devvpc7-r4.8xlarge-v072;stack:devvpc7;zone:us-east-1c;itype:r4.8xlarge;cluster:titusagent-devvpc7-r4.8xlarge;id:i-0d736710f99b183f1;res:ResourceSet-ENIs-7-29;distcodename:xenial;distrelease:16.04\n\n\n\n\nOf course, adapt the configuration file to your needs. The Mesos attributes are broken down as below:\n\n\n\n\nregion: AWS region\n\n\nasg: AWS ASG name\n\n\nstack: Which (isolated) Titus environment\n\n\nzone: Which AWS availabilityzone\n\n\nitype: What AWS instance type\n\n\ncluster: This is an internal naming convention of titusagent-${stack}-${custom}\n\n\nid: EC2 instance identifier\n\n\nres: Special construction to signal to the Titus master how many ENIs this instance type can handle, for example \"ResourceSet-ENIs-7-29\", means that this machine can have 7 additional ENIs, and 29 IPs per ENI.\n\n\n\n\nVPC Driver\n\n\nInstall the VPC Driver from the repo. The name of the package is \ntitus-vpc-driver\n.\n\n Add \nnet.ifnames=0\n to GRUB_CMDLINE_LINUX= in \n/etc/default/grub\n and then run \nsudo update-grub\n.\n\n Remove the \n/etc/udev/rules.d/70-persistent-net.rules\n file and then run \nsudo reboot\n.\n* After reboot, the default interface should be named \neth0\n instead of the previous \nens3\n.\n\n\ntitus-executor\n\n\nInstall the VPC Driver from the repo. The name of the package is \ntitus-executor\n.\n\n\nYou need to configure it with a configuration file at \n/etc/titus-executor/config.json\n. The configuration file looks like below:\n\n\n{\n  \nstack\n: \nmainvpc\n,\n  \ndocker\n: {\n    \nhost\n: \ntcp://127.0.0.1:4243\n,\n    \nregistry\n: \ndocker.io\n\n  },\n  \nuploaders\n: {\n     },\n  \nenv\n: {\n    \ncopiedFromHost\n: [\n      \nNETFLIX_ENVIRONMENT\n,\n      \nNETFLIX_ACCOUNT\n,\n      \nNETFLIX_STACK\n,\n      \nEC2_INSTANCE_ID\n,\n      \nEC2_REGION\n,\n      \nEC2_AVAILABILITY_ZONE\n,\n      \nEC2_OWNER_ID\n,\n      \nEC2_VPC_ID\n,\n      \nEC2_RESERVATION_ID\n\n    ],\n    \nhardCoded\n: {\n      \nNETFLIX_APPUSER\n: \nappuser\n,\n      \nEC2_DOMAIN\n: \namazonaws.com\n\n    }\n  },\n  \nstatusCheckFrequency\n: \n10s\n,\n  \nuseNewNetworkDriver\n: true,\n  \nusePrivilegedTasks\n: false,\n  \nuseMetatron\n: true,\n  \nlogsTmpDir\n: \n/var/lib/titus-container-logs\n\n}", 
            "title": "Agent"
        }, 
        {
            "location": "/install/agent/#using-cloud-init", 
            "text": "We recommend that you use the following cloud-init config with Ubuntu 16.04. Just make sure you've added an IAM role to your profile:  cloud-init . There are a few fields we recommend filling in for yourself here. For example, you should fill in your SSH keys, and not ours. Also, you should configure the  /etc/mesos-attributes.sh  config file that's included in the cloud-init to be suited for your instance. The mesos-specific configuration that needs to be adjusted is the  stack ,  asg , and  Res  attributes. You will also need to edit the run_cmd section, and change  zk://172.31.0.136:2181/titus/mainvpc/mesos , based on your Zookeeper configuration. The specifics of the  Res  attribute, and how it refers to how your ENIs work is explained below.  In order to use cloud-init, you need to go ahead and paste the linked yaml file into your Ubuntu 16.04 VM's user data.", 
            "title": "Using Cloud-init"
        }, 
        {
            "location": "/install/agent/#manual-install", 
            "text": "This is for advanced users only. We do not recommend using these instructions unless you're running a customized system. We also recommend looking at the cloud-init as the reference for canonical configuration files.", 
            "title": "Manual Install"
        }, 
        {
            "location": "/install/agent/#linux", 
            "text": "We recommend Ubuntu Linux, Xenial, with a 4.4 or newer version for running Titus. We also recommend this for developing Titus agent itself. We recommend using XFS or Btrfs for the storage of the Docker daemon itself, and separating out the storage of the agent \"Control Plane\" components versus the non-control plane components.", 
            "title": "Linux"
        }, 
        {
            "location": "/install/agent/#install-packagecloud-repo", 
            "text": "Run the following commands:  curl -s https://8095c452e9473a3fae3ea86a6f2572c2cde0d7b5ec63e84f:@packagecloud.io/install/repositories/netflix/titus/script.deb.sh | sudo bash\napt-get update", 
            "title": "Install Packagecloud Repo"
        }, 
        {
            "location": "/install/agent/#install-docker", 
            "text": "Install docker as instructed  here   Add the current user to docker group  sudo gpasswd -a $USER docker , log out and then log back in.   You can add the following drop-in systemd unit to configure Docker for your needs:  # /etc/systemd/system/docker.service.d/10-docker-config.conf\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd --log-level=debug -H fd:// --init-path=/apps/titus-executor/bin/tini-static --iptables=false --storage-driver=overlay2\nRestart=always\nStartLimitInterval=0\nRestartSec=5  You should also configure the Docker TCP Proxy, with the following systemd units:", 
            "title": "Install Docker"
        }, 
        {
            "location": "/install/agent/#docker-tcp-proxyservice", 
            "text": "# /lib/systemd/system/docker-tcp-proxy.service\n[Unit]\nDescription=Docker Unix Socket TCP Proxy\nRequires=docker.socket\nRequires=docker-tcp-proxy.socket\nAfter=docker.socket\nAfter=docker-tcp-proxy.socket\n\n[Service]\nExecStart=/lib/systemd/systemd-socket-proxyd /var/run/docker.sock\n\n[Install]\nWantedBy=multi-user.target", 
            "title": "docker-tcp-proxy.service"
        }, 
        {
            "location": "/install/agent/#docker-tcp-proxysocket", 
            "text": "# /lib/systemd/system/docker-tcp-proxy.socket\n[Unit]\nRequires=docker.socket\nAfter=docker.socket\n\n[Socket]\nListenStream=0.0.0.0:4243\n\n[Install]\nWantedBy=sockets.target  Once you've done this, we recommend restarting docker, enabling these systemd units to restart on every boot.", 
            "title": "docker-tcp-proxy.socket"
        }, 
        {
            "location": "/install/agent/#mesos-agent", 
            "text": "You can download Mesos from Mesosphere's OSS site here: https://open.mesosphere.com/downloads/mesos/#apache-mesos-1.1.3, we recommend installing 1.1.3. You can download the deb directly  http://repos.mesosphere.com/ubuntu/pool/main/m/mesos/mesos_1.1.3-2.0.1.ubuntu1604_amd64.deb .  Unfortunately, Mesos does not come with a systemd unit file out of the box, so we have one that you can adapt to your own needs:  # /lib/systemd/system/mesos-agent.service\n[Unit]\nDescription=Mesos\nWants=docker.service\nAfter=docker.service\nConflicts=halt.target shutdown.target sigpwr.target\n\n[Service]\nExecStartPre=/bin/mkdir -p /var/lib/mesos\nEnvironmentFile=/etc/mesos-agent.config\nExecStart=/usr/sbin/mesos-agent\nExecStopPost=/bin/rm -rf /var/lib/mesos/meta/slaves/latest\nRestart=always\nStartLimitInterval=0\nRestartSec=5\nLimitNOFILE=65535\nKillMode=mixed\nKillSignal=SIGUSR1\n\n[Install]\nWantedBy=multi-user.target  Our  /etc/mesos-agent.config  includes the following:  MESOS_MASTER=zk://1.1.1.1:2181,2.2.2.2:2181/titus/devvpc7/mesos\nMESOS_PORT=7101\nMESOS_RECOVER=reconnect\nMESOS_WORK_DIR=/var/lib/mesos\nMESOS_STRICT=false\nMESOS_RESOURCES=ports:[7150-7200];mem:230400;disk:523264;network:9000\nMESOS_ATTRIBUTES=region:us-east-1;asg:titusagent-devvpc7-r4.8xlarge-v072;stack:devvpc7;zone:us-east-1c;itype:r4.8xlarge;cluster:titusagent-devvpc7-r4.8xlarge;id:i-0d736710f99b183f1;res:ResourceSet-ENIs-7-29;distcodename:xenial;distrelease:16.04  Of course, adapt the configuration file to your needs. The Mesos attributes are broken down as below:   region: AWS region  asg: AWS ASG name  stack: Which (isolated) Titus environment  zone: Which AWS availabilityzone  itype: What AWS instance type  cluster: This is an internal naming convention of titusagent-${stack}-${custom}  id: EC2 instance identifier  res: Special construction to signal to the Titus master how many ENIs this instance type can handle, for example \"ResourceSet-ENIs-7-29\", means that this machine can have 7 additional ENIs, and 29 IPs per ENI.", 
            "title": "Mesos-agent"
        }, 
        {
            "location": "/install/agent/#vpc-driver", 
            "text": "Install the VPC Driver from the repo. The name of the package is  titus-vpc-driver .  Add  net.ifnames=0  to GRUB_CMDLINE_LINUX= in  /etc/default/grub  and then run  sudo update-grub .  Remove the  /etc/udev/rules.d/70-persistent-net.rules  file and then run  sudo reboot .\n* After reboot, the default interface should be named  eth0  instead of the previous  ens3 .", 
            "title": "VPC Driver"
        }, 
        {
            "location": "/install/agent/#titus-executor", 
            "text": "Install the VPC Driver from the repo. The name of the package is  titus-executor .  You need to configure it with a configuration file at  /etc/titus-executor/config.json . The configuration file looks like below:  {\n   stack :  mainvpc ,\n   docker : {\n     host :  tcp://127.0.0.1:4243 ,\n     registry :  docker.io \n  },\n   uploaders : {\n     },\n   env : {\n     copiedFromHost : [\n       NETFLIX_ENVIRONMENT ,\n       NETFLIX_ACCOUNT ,\n       NETFLIX_STACK ,\n       EC2_INSTANCE_ID ,\n       EC2_REGION ,\n       EC2_AVAILABILITY_ZONE ,\n       EC2_OWNER_ID ,\n       EC2_VPC_ID ,\n       EC2_RESERVATION_ID \n    ],\n     hardCoded : {\n       NETFLIX_APPUSER :  appuser ,\n       EC2_DOMAIN :  amazonaws.com \n    }\n  },\n   statusCheckFrequency :  10s ,\n   useNewNetworkDriver : true,\n   usePrivilegedTasks : false,\n   useMetatron : true,\n   logsTmpDir :  /var/lib/titus-container-logs \n}", 
            "title": "titus-executor"
        }, 
        {
            "location": "/test/batch/", 
            "text": "To send requests, curl jobs to the gateway node\n\n\nTesting the most basic job\n\n\ncurl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{\n   \napplicationName\n: \nlibrary/ubuntu\n,\n   \nversion\n: \nlatest\n,\n   \ntype\n: \nbatch\n,\n   \nentryPoint\n: \nsleep 10\n,\n   \ninstances\n: 1,\n   \ncpu\n: 1,\n   \nmemory\n: 1024,\n   \ndisk\n: 1000,\n   \nnetworkMbps\n: 128\n }' 'http://GATEWAYIP:7001/api/v2/jobs'\n\n\n\n\nTesting with IAM Roles\n\n\ncurl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{\n   \napplicationName\n: \nlibrary/ubuntu\n,\n   \nversion\n: \nlatest\n,\n   \ntype\n: \nbatch\n,\n   \nentryPoint\n: \nsleep 10\n,\n   \ninstances\n: 1,\n   \ncpu\n: 1,\n   \nmemory\n: 1024,\n   \ndisk\n: 1000,\n   \nnetworkMbps\n: 128,\n   \niamProfile\n: \narn:aws:iam::ACCOUNTID:role/IAMPROFILENAME\n \n }' 'http://GATEWAYIP:7001/api/v2/jobs'\n\n\n\n\nTesting with security groups and routable IP's\n\n\ncurl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{\n   \napplicationName\n: \nlibrary/ubuntu\n,\n   \nversion\n: \nlatest\n,\n   \ntype\n: \nbatch\n,\n   \nentryPoint\n: \nsleep 10\n,\n   \ninstances\n: 1,\n   \ncpu\n: 1,\n   \nmemory\n: 1024,\n   \ndisk\n: 1000,\n   \nnetworkMbps\n: 128,\n   \nallocateIpAddress\n: true,\n   \nsecurityGroups\n: [\nsg-34b11b52\n]\n}' 'http://GATEWAYIP:7001/api/v2/jobs'\n\n\n\n\nTesting the metadataservice without executor or VPC driver\n\n\nThere is a a testing flag you can add to the metadata-service run command \nexport ALLOW_REMOTE_IP_OVERRIDE=true\n\nwhich will allow you to override the connection IP used to lookup the container doing a metadata request. If you\nstart two containers with the below labels:\n\n\nsudo docker run -d \\\n  --label TITUS_TASK_INSTANCE_ID=abf580cb-740b-4ab0-a46a-ebb5d590dbfe \\\n  --label ec2.iam.role=arn:aws:iam::ACCOUNTID:role/titusappwiths3InstanceProfile \\\n  --label titus.net.ipv4=1.1.1.1 \\\n  --label titus.task_id=Titus-15125145-worker-0-52 \\\n  --label titus.vpc.ipv4=1.1.1.1 \\\n  ubuntu:latest sleep 1000000\n\nsudo docker run -d \\\n  --label TITUS_TASK_INSTANCE_ID=abf580cb-740b-4ab0-a46a-ebb5d590dbfe \\\n  --label ec2.iam.role=arn:aws:iam::ACCOUNTID:role/titusappnos3InstanceProfile \\\n  --label titus.net.ipv4=1.1.1.2 \\\n  --label titus.task_id=Titus-15125145-worker-0-52 \\\n  --label titus.vpc.ipv4=1.1.1.2 \\\n  ubuntu:latest sleep 1000000\n\n\n\n\nYou can test the IAM support by running curl with a special header:\n\n\ncurl --header \nremote-ip:1.1.1.1\n http://10.11.10.11:9999/latest/meta-data/iam/security-credentials/titusappwiths3InstanceProfile\n\ncurl --header \nremote-ip:1.1.1.2\n http://10.11.10.11:9999/latest/meta-data/iam/security-credentials/titusappnos3InstanceProfile\n\n\n\n\nDebugging Mesos\n\n\n\n\nIPOFMESOS:5050\n\n\nIPOFMESOS:5050/slaves", 
            "title": "Basic batch task"
        }, 
        {
            "location": "/test/batch/#testing-the-most-basic-job", 
            "text": "curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{\n    applicationName :  library/ubuntu ,\n    version :  latest ,\n    type :  batch ,\n    entryPoint :  sleep 10 ,\n    instances : 1,\n    cpu : 1,\n    memory : 1024,\n    disk : 1000,\n    networkMbps : 128\n }' 'http://GATEWAYIP:7001/api/v2/jobs'", 
            "title": "Testing the most basic job"
        }, 
        {
            "location": "/test/batch/#testing-with-iam-roles", 
            "text": "curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{\n    applicationName :  library/ubuntu ,\n    version :  latest ,\n    type :  batch ,\n    entryPoint :  sleep 10 ,\n    instances : 1,\n    cpu : 1,\n    memory : 1024,\n    disk : 1000,\n    networkMbps : 128,\n    iamProfile :  arn:aws:iam::ACCOUNTID:role/IAMPROFILENAME  \n }' 'http://GATEWAYIP:7001/api/v2/jobs'", 
            "title": "Testing with IAM Roles"
        }, 
        {
            "location": "/test/batch/#testing-with-security-groups-and-routable-ips", 
            "text": "curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{\n    applicationName :  library/ubuntu ,\n    version :  latest ,\n    type :  batch ,\n    entryPoint :  sleep 10 ,\n    instances : 1,\n    cpu : 1,\n    memory : 1024,\n    disk : 1000,\n    networkMbps : 128,\n    allocateIpAddress : true,\n    securityGroups : [ sg-34b11b52 ]\n}' 'http://GATEWAYIP:7001/api/v2/jobs'", 
            "title": "Testing with security groups and routable IP's"
        }, 
        {
            "location": "/test/batch/#testing-the-metadataservice-without-executor-or-vpc-driver", 
            "text": "There is a a testing flag you can add to the metadata-service run command  export ALLOW_REMOTE_IP_OVERRIDE=true \nwhich will allow you to override the connection IP used to lookup the container doing a metadata request. If you\nstart two containers with the below labels:  sudo docker run -d \\\n  --label TITUS_TASK_INSTANCE_ID=abf580cb-740b-4ab0-a46a-ebb5d590dbfe \\\n  --label ec2.iam.role=arn:aws:iam::ACCOUNTID:role/titusappwiths3InstanceProfile \\\n  --label titus.net.ipv4=1.1.1.1 \\\n  --label titus.task_id=Titus-15125145-worker-0-52 \\\n  --label titus.vpc.ipv4=1.1.1.1 \\\n  ubuntu:latest sleep 1000000\n\nsudo docker run -d \\\n  --label TITUS_TASK_INSTANCE_ID=abf580cb-740b-4ab0-a46a-ebb5d590dbfe \\\n  --label ec2.iam.role=arn:aws:iam::ACCOUNTID:role/titusappnos3InstanceProfile \\\n  --label titus.net.ipv4=1.1.1.2 \\\n  --label titus.task_id=Titus-15125145-worker-0-52 \\\n  --label titus.vpc.ipv4=1.1.1.2 \\\n  ubuntu:latest sleep 1000000  You can test the IAM support by running curl with a special header:  curl --header  remote-ip:1.1.1.1  http://10.11.10.11:9999/latest/meta-data/iam/security-credentials/titusappwiths3InstanceProfile\n\ncurl --header  remote-ip:1.1.1.2  http://10.11.10.11:9999/latest/meta-data/iam/security-credentials/titusappnos3InstanceProfile", 
            "title": "Testing the metadataservice without executor or VPC driver"
        }, 
        {
            "location": "/test/batch/#debugging-mesos", 
            "text": "IPOFMESOS:5050  IPOFMESOS:5050/slaves", 
            "title": "Debugging Mesos"
        }, 
        {
            "location": "/faq/", 
            "text": "", 
            "title": "FAQ"
        }
    ]
}